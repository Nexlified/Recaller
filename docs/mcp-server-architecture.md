# MCP Server Architecture for Recaller

## Overview

The Model Context Protocol (MCP) server provides a standardized interface for Large Language Model (LLM) integration in Recaller. It implements MCP v1 protocol to enable secure, privacy-first connections with local or self-hosted LLMs while maintaining Recaller's multi-tenant architecture and privacy requirements.

## ğŸ”’ Security & Privacy Features (Enhanced)

### Tenant Isolation
- **Request Context**: All requests include tenant identification  
- **Data Separation**: No cross-tenant data access - models are scoped by tenant ID
- **Model Access**: Tenant-specific model permissions and ownership validation
- **Resource Limits**: Per-tenant rate limiting and quotas
- **API Filtering**: All endpoints filter results by tenant context

### Privacy Protection
- **On-Device Processing**: Local model execution preferred
- **External Request Blocking**: Configurable blocking of all external network requests
- **URL Validation**: Automatic detection and blocking of external URLs in configurations and prompts
- **Data Sanitization**: Automatic removal of sensitive data from logs and error messages
- **No Data Retention**: Optional zero-retention policy (default: 0 days)
- **Request Anonymization**: Optional log anonymization with sensitive pattern detection
- **Secure Communication**: TLS/HTTPS for all communications
- **Local-Only Enforcement**: Strict mode to ensure no data leaves the environment

### Access Control
- **API Authentication**: Token-based access control with tenant validation
- **Model Permissions**: Fine-grained model access control per tenant
- **Rate Limiting**: Protection against abuse with tenant-scoped limits
- **Resource Quotas**: CPU/memory usage limits per tenant
- **Configuration Validation**: Privacy-compliant model configuration validation

## Architecture Design

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend Layer                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     FastAPI Backend                            â”‚
â”‚                  (Recaller Main App)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ HTTP API / Internal Communication
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MCP Server                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Model Registry       â€¢ Inference Service                    â”‚
â”‚  â€¢ Protocol Handler     â€¢ Authentication                       â”‚
â”‚  â€¢ Tenant Isolation     â€¢ Health Monitoring                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚ MCP v1 Protocol
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Model Backends                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Ollama              â€¢ HuggingFace                           â”‚
â”‚  â€¢ OpenAI Compatible   â€¢ Local Models                          â”‚
â”‚  â€¢ Custom Backends     â€¢ Cloud APIs                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Service Boundaries

#### 1. **MCP Server Core** (`backend/mcp_server/`)
- **Responsibility**: MCP protocol implementation, request routing, and coordination
- **Key Components**:
  - Protocol handler for MCP v1 messages
  - Request/response lifecycle management
  - Error handling and validation
  - WebSocket/HTTP transport layers

#### 2. **Model Registry** (`backend/mcp_server/models/`)
- **Responsibility**: Model lifecycle management and discovery
- **Key Components**:
  - Model registration and configuration
  - Backend abstraction layer
  - Health monitoring and status tracking
  - Capability discovery and validation

#### 3. **Inference Service** (`backend/mcp_server/services/`)
- **Responsibility**: Request processing and inference coordination
- **Key Components**:
  - Rate limiting and resource management
  - Context validation and preprocessing
  - Backend-specific inference handling
  - Response formatting and post-processing

#### 4. **Authentication & Authorization** (`backend/mcp_server/services/auth.py`)
- **Responsibility**: Security and tenant isolation
- **Key Components**:
  - Tenant verification and context injection
  - API access control and permissions
  - Integration with main backend authentication
  - Privacy and security enforcement

#### 5. **Configuration Management** (`backend/mcp_server/config/`)
- **Responsibility**: Settings and configuration management
- **Key Components**:
  - Environment-based configuration
  - Model backend configurations
  - Security and privacy settings
  - Resource limits and constraints

## Directory Structure

```
backend/mcp_server/
â”œâ”€â”€ __init__.py                 # Package initialization
â”œâ”€â”€ main.py                     # FastAPI application and server startup
â”œâ”€â”€ requirements.txt            # MCP server specific dependencies
â”œâ”€â”€ Dockerfile                  # Container configuration
â”œâ”€â”€ .env.example               # Environment configuration template
â”‚
â”œâ”€â”€ core/                      # Core MCP protocol implementation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ protocol.py            # MCP v1 protocol handler and server
â”‚
â”œâ”€â”€ models/                    # Model management and backends
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ registry.py           # Model registry and backend abstractions
â”‚
â”œâ”€â”€ services/                  # Business logic services
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ inference.py          # Inference processing service
â”‚   â””â”€â”€ auth.py               # Authentication and authorization
â”‚
â”œâ”€â”€ api/                       # REST API endpoints
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ endpoints.py          # API route definitions
â”‚
â”œâ”€â”€ schemas/                   # Data models and validation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ mcp_schemas.py        # Pydantic schemas for MCP protocol
â”‚
â””â”€â”€ config/                    # Configuration management
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ settings.py           # Application settings
    â””â”€â”€ models.example.json   # Example model configurations
```

## Technology Stack

### Core Technologies
- **Language**: Python 3.11+
- **Framework**: FastAPI (async/await support)
- **Protocol**: MCP v1 (Model Context Protocol)
- **Transport**: WebSocket and HTTP
- **Validation**: Pydantic schemas
- **Documentation**: OpenAPI/Swagger

### Model Backend Support
- **Ollama**: Local LLM deployment platform
- **HuggingFace**: Transformers and model hub integration
- **OpenAI Compatible**: APIs following OpenAI format
- **Local Models**: Direct PyTorch/TensorFlow integration
- **Custom Backends**: Extensible backend architecture

### Integration Technologies
- **HTTP Client**: httpx for async backend communication
- **WebSockets**: websockets library for MCP protocol
- **Authentication**: JWT integration with main backend
- **Monitoring**: Built-in health checks and metrics

## Integration Points

### 1. **FastAPI Backend Integration**
- **Authentication**: Shares authentication context with main backend
- **Tenant Context**: Inherits tenant isolation from main application
- **Configuration**: Integrated configuration management
- **Monitoring**: Shared health check and monitoring infrastructure

### 2. **LLM Model Integration**
- **Protocol Standardization**: MCP v1 for consistent communication
- **Backend Abstraction**: Pluggable backend architecture
- **Capability Discovery**: Automatic detection of model capabilities
- **Health Monitoring**: Continuous model availability checks

### 3. **Database Integration**
- **Model Registry**: Persistent model configuration storage
- **Tenant Data**: Respects existing tenant isolation
- **Configuration**: Shared database for model metadata
- **Audit Logs**: Optional request/response logging

## Security and Privacy Features

### Tenant Isolation
- **Request Context**: All requests include tenant identification
- **Data Separation**: No cross-tenant data access
- **Model Access**: Tenant-specific model permissions
- **Resource Limits**: Per-tenant rate limiting and quotas

### Privacy Protection
- **On-Device Processing**: Local model execution preferred
- **No Data Retention**: Optional zero-retention policy
- **Request Anonymization**: Optional log anonymization
- **Secure Communication**: TLS/HTTPS for all communications

### Access Control
- **API Authentication**: Token-based access control
- **Model Permissions**: Fine-grained model access control
- **Rate Limiting**: Protection against abuse
- **Resource Quotas**: CPU/memory usage limits

## Configuration

### Environment Variables
Key configuration options for the MCP server:

```bash
# Server Configuration
MCP_SERVER_HOST=0.0.0.0
MCP_SERVER_PORT=8001
MCP_PROTOCOL_VERSION=1.0.0

# Security & Tenant Isolation
MCP_SECRET_KEY=your-secret-key
MCP_ENABLE_TENANT_ISOLATION=true

# Backend Integration
BACKEND_API_URL=http://backend:8000
BACKEND_API_KEY=optional-api-key

# Resource Limits
MAX_CONCURRENT_REQUESTS=10
MAX_CONTEXT_LENGTH=4096
MAX_RESPONSE_TOKENS=1024

# Privacy & Security (Enhanced)
ENABLE_REQUEST_LOGGING=false
ANONYMIZE_LOGS=true
DATA_RETENTION_DAYS=0
BLOCK_EXTERNAL_REQUESTS=true
ALLOWED_EXTERNAL_HOSTS=[]
ENFORCE_LOCAL_ONLY=true
ANONYMIZE_ERROR_MESSAGES=true
```

## ğŸ” Privacy Enforcement

The MCP server includes comprehensive privacy protection:

1. **External Request Blocking**: All external network requests are blocked by default
2. **Local Address Detection**: Automatic allowlisting of local/private network addresses
3. **URL Validation**: Scanning of model configurations and prompts for external URLs
4. **Data Sanitization**: Automatic removal of emails, SSNs, credit cards, IPs from logs
5. **Error Message Sanitization**: Removal of file paths and URLs from error messages
6. **Configurable Whitelist**: Optional whitelist for trusted external hosts

### Privacy Status Monitoring

Check current privacy settings via the API:

```bash
GET /api/v1/privacy/status
```

Returns current privacy enforcement configuration and status.

### Model Configuration
Models are configured through JSON files or API registration:

```json
{
  "name": "Llama 3.2 3B",
  "backend_type": "ollama",
  "config": {
    "base_url": "http://localhost:11434",
    "model_name": "llama3.2:3b"
  },
  "capabilities": ["completion", "chat"]
}
```

## Deployment

### Docker Compose Integration
The MCP server is integrated into the existing docker-compose setup:

```yaml
mcp_server:
  build: ./backend/mcp_server
  ports:
    - "8001:8001"
  environment:
    - BACKEND_API_URL=http://backend:8000
    - MCP_ENABLE_TENANT_ISOLATION=true
  depends_on:
    - backend
    - redis
```

### Standalone Deployment
For development or testing:

```bash
cd backend/mcp_server
pip install -r requirements.txt
python main.py
```

## API Endpoints

### Model Management
- `POST /api/v1/models/register` - Register new model (tenant-scoped)
- `GET /api/v1/models` - List available models (filtered by tenant)
- `GET /api/v1/models/{id}` - Get model information (tenant access control)
- `DELETE /api/v1/models/{id}` - Unregister model (tenant ownership validation)

### Inference
- `POST /api/v1/inference/completion` - Text completion (tenant + privacy validation)
- `POST /api/v1/inference/chat` - Chat completion (tenant + privacy validation)
- `POST /api/v1/inference/embedding` - Text embeddings (tenant + privacy validation)

### Health and Monitoring
- `GET /api/v1/health` - Overall health check
- `GET /api/v1/health/models/{id}` - Model-specific health
- `GET /api/v1/stats` - Server statistics (tenant-scoped)
- `GET /api/v1/info` - Server information

### Privacy & Security
- `GET /api/v1/privacy/status` - Privacy enforcement status

## Extensibility

### Adding New Backends
1. Implement the `ModelBackend` interface
2. Register in `_backend_classes` mapping
3. Add configuration schema
4. Implement health checks and inference methods

### Custom Inference Types
1. Add new `InferenceType` enum value
2. Create request/response schemas
3. Implement service logic
4. Add API endpoint

### Protocol Extensions
1. Extend MCP schemas for new message types
2. Register handlers in protocol manager
3. Update API documentation
4. Maintain backward compatibility

## Future Considerations

### Scalability
- **Horizontal Scaling**: Load balancing across multiple instances
- **Model Sharding**: Distribute models across servers
- **Caching**: Response caching for repeated requests
- **Queue Management**: Async request processing

### Advanced Features
- **Model Versioning**: Support for multiple model versions
- **A/B Testing**: Model comparison and testing
- **Fine-tuning**: Support for model customization
- **Federated Learning**: Multi-tenant model training

### Integration Enhancements
- **Real-time Streaming**: WebSocket streaming responses
- **Batch Processing**: Bulk inference requests
- **Model Marketplace**: Community model sharing
- **Plugin System**: Third-party extensions